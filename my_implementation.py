# -*- coding: utf-8 -*-
"""my_implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FytjAVmoN7Dz8kcR4dPNntyqcvTikbAb
"""

from __future__ import print_function
from tensorflow.keras.models import Model, load_model, Sequential
from tensorflow.keras.layers import Input, LSTM, Dense, Embedding
import numpy as np
import re

num_encoder_tokens = 1430
num_decoder_tokens = 1223
batch_size = 64  # Batch size for training.
epochs = 50 # Number of epochs to train for.
latent_dim = 256  # Latent dimensionality of the encoding space.
num_samples = 5000  # Number of samples to train on.
# Path to the data txt file on disk.
data_path = 'new4.txt'
mal_text = []
eng_text = []

# Read 
f1=open("new4.txt","r")
full_text = f1.read()
#x=re.sub(" ","\n",full_text)
x=" ".join(re.split("[^a-zA-Z]*", full_text)) 
y=re.findall(u'[\u0d00-\u0d7f]+', full_text) 
eng_text = x.split(' ')
mal_text = y

#Mapping input and output
model = Sequential()
encoder = model.add(Dense(num_encoder_tokens, activation='softmax'))
decoder = model.add(Dense(num_decoder_tokens, activation='softmax'))


model.compile(optimizer='rmsprop', loss='categorical_crossentropy',
              metrics=['accuracy'])

model.fit(np.array(mal_text), np.array(eng_text), epochs, batch_size, validation_split=0.2)


# split the english text and malayalam text to words
print(type(mal_text))
print(type(eng_text))